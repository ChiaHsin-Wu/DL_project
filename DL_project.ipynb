{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "2steg3mUpayM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KLsetN4mIEDT",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "!pip install kaggle\n",
        "!pip install fasttext"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p ~/.kaggle\n",
        "!cp '/content/drive/MyDrive/Colab Notebooks/kaggle.json' ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "!kaggle datasets download nadyinky/sephora-products-and-skincare-reviews\n",
        "!unzip '/content/sephora-products-and-skincare-reviews.zip'"
      ],
      "metadata": {
        "id": "iHLRG256l0lo",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os, shutil\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "26saHyW_nWjX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "product_info = pd.read_csv('product_info.csv', sep=',')\n",
        "print('original shape = ', product_info.shape)\n",
        "keep_cols = [\n",
        "  'product_id', 'product_name', 'ingredients', 'highlights',\n",
        "  'primary_category', 'secondary_category', 'tertiary_category'\n",
        "]\n",
        "product_info = product_info[keep_cols]\n",
        "required_cols = ['product_id', 'product_name', 'ingredients', 'highlights']\n",
        "product_info = product_info.dropna(subset=required_cols)\n",
        "product_info.drop_duplicates(subset='product_id', inplace=True)\n",
        "print('cleaned shape = ', product_info.shape)"
      ],
      "metadata": {
        "id": "GyYWn0dWDTNH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_review_file(path, name):\n",
        "  df = pd.read_csv(path, usecols=lambda col: col in columns_to_keep)\n",
        "  print(f\"{name}: original shape = {df.shape}\")\n",
        "  df = df.dropna()\n",
        "  df['is_recommended'] = df['is_recommended'].astype(int)\n",
        "  return df\n",
        "\n",
        "columns_to_keep = ['is_recommended', 'review_text', 'product_id']\n",
        "review_paths = {\n",
        "  '0-250': 'reviews_0-250.csv',\n",
        "  '250-500': 'reviews_250-500.csv',\n",
        "  '500-750': 'reviews_500-750.csv',\n",
        "  '750-1250': 'reviews_750-1250.csv',\n",
        "  '1250-end': 'reviews_1250-end.csv'\n",
        "}\n",
        "\n",
        "cleaned_reviews = {}\n",
        "for name, path in review_paths.items():\n",
        "  df = clean_review_file(path, name)\n",
        "  cleaned_reviews[name] = df\n",
        "  print(f\"{name}: cleaned shape = {df.shape}\")\n",
        "\n",
        "all_reviews_df = pd.concat(cleaned_reviews.values(), ignore_index=True)\n",
        "positive_reviews = all_reviews_df[all_reviews_df['is_recommended'] == 1].copy()\n",
        "negative_reviews = all_reviews_df[all_reviews_df['is_recommended'] == 0].copy()\n",
        "print('positive reviews shape = ', positive_reviews.shape)\n",
        "print('negative reviews shape = ', negative_reviews.shape)"
      ],
      "metadata": {
        "id": "_2V5B2pJ_CEy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import KMeans\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "from sklearn.metrics import classification_report, f1_score\n",
        "from sklearn.utils import shuffle\n",
        "from transformers import BertTokenizer, BertModel\n",
        "from tensorflow.keras.metrics import AUC\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from keras.metrics import Precision, Recall, BinaryAccuracy\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding, Flatten, Dense, Input, SimpleRNN, Dropout\n",
        "from keras.optimizers import RMSprop\n",
        "from keras import layers, regularizers\n",
        "from nltk.stem import PorterStemmer\n",
        "from collections import Counter\n",
        "import torch\n",
        "import numpy as np\n",
        "import re\n",
        "import matplotlib.pyplot as plt\n",
        "import fasttext\n",
        "import ast"
      ],
      "metadata": {
        "id": "wW5IFcvWaCkF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_top_terms_by_cluster(\n",
        "    texts, top_n=300, n_clusters=10, max_features=1000, min_df=5,\n",
        "    max_df=0.7, stop_words='english', ngram_range=(1, 2), model_name='all-MiniLM-L6-v2'):\n",
        "\n",
        "  # 1. TF-IDF 向量化\n",
        "  vectorizer = TfidfVectorizer(\n",
        "    stop_words=stop_words,\n",
        "    max_df=max_df,\n",
        "    min_df=min_df,\n",
        "    max_features=max_features,\n",
        "    ngram_range=ngram_range\n",
        "  )\n",
        "  X = vectorizer.fit_transform(texts)\n",
        "  feature_names = vectorizer.get_feature_names_out()\n",
        "\n",
        "  # 2. 取出前 top_n 高分詞\n",
        "  tfidf_scores = X.sum(axis=0).A1\n",
        "  sorted_items = sorted(zip(tfidf_scores, feature_names), reverse=True)\n",
        "  top_terms = [word for _, word in sorted_items[:top_n]]\n",
        "\n",
        "  # 3. BERT 向量\n",
        "  model = SentenceTransformer(model_name)\n",
        "  embeddings = model.encode(top_terms)\n",
        "\n",
        "  # 4. KMeans 聚類\n",
        "  kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
        "  labels = kmeans.fit_predict(embeddings)\n",
        "\n",
        "  # 5. 整理群組關鍵詞\n",
        "  clusters = {}\n",
        "  for idx, label in enumerate(labels):\n",
        "    clusters.setdefault(label, []).append(top_terms[idx])\n",
        "\n",
        "  return clusters"
      ],
      "metadata": {
        "id": "Fc6RD-OFZC9k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "positive_clusters = extract_top_terms_by_cluster(positive_reviews['review_text'])\n",
        "negative_clusters = extract_top_terms_by_cluster(negative_reviews['review_text'])"
      ],
      "metadata": {
        "id": "Rl-cKbcQZSlA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for cid, words in positive_clusters.items():\n",
        "  print(f\"[Positive Cluster {cid}], keywords: {words}\")\n",
        "\n",
        "for cid, words in negative_clusters.items():\n",
        "  print(f\"[Negative Cluster {cid}], keywords: {words}\")"
      ],
      "metadata": {
        "id": "MnLLSBkIEB-S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cluster_id_to_name_pos = {\n",
        "  0: \"Texture & Radiant Skin\",\n",
        "  1: \"Moisturizing & Gentle\",\n",
        "  2: \"Strong Recommendation\"\n",
        "}\n",
        "\n",
        "keyword_to_tag_pos = {\n",
        "  # Cluster 0 – Texture & Radiant Skin\n",
        "  \"soft\": 0,\n",
        "  \"glow\": 0,\n",
        "  \"brighter\": 0,\n",
        "  \"radiant\": 0,\n",
        "  \"tone\": 0,\n",
        "  \"even skin\": 0,\n",
        "  \"smooth\": 0,\n",
        "  \"plump\": 0,\n",
        "  \"dewy\": 0,\n",
        "\n",
        "  # Cluster 1 – Moisturizing & Gentle\n",
        "  \"moisturize\": 1,\n",
        "  \"hydrate\": 1,\n",
        "  \"gentle\": 1,\n",
        "  \"soothing\": 1,\n",
        "  \"sensitive skin\": 1,\n",
        "  \"acne prone\": 1,\n",
        "  \"combination skin\": 1,\n",
        "  \"non irritating\": 1,\n",
        "  \"absorbs\": 1,\n",
        "  \"non greasy\": 1,\n",
        "  \"lightweight\": 1,\n",
        "  'dry': 1,\n",
        "\n",
        "  # Cluster 2 – Strong Recommendation\n",
        "  \"love\": 2,\n",
        "  \"recommend\": 2,\n",
        "  \"favorite\": 2,\n",
        "  \"best\": 2,\n",
        "  \"wonderful\": 2\n",
        "}\n",
        "\n",
        "cluster_id_to_name_neg = {\n",
        "  0: \"No Visible Change\",\n",
        "  1: \"Scent\",\n",
        "  2: \"Ineffectiveness\",\n",
        "  3: \"Specific Area or Routine Issues\",\n",
        "  4: \"Texture and Hydration Problems\",\n",
        "  5: \"Skin Type Sensitivity\"\n",
        "}\n",
        "\n",
        "keyword_to_tag_neg = {\n",
        "  # Cluster 0 – No Visible Change\n",
        "  \"no difference\": 0,\n",
        "  \"time\": 0,\n",
        "  \"little\": 0,\n",
        "  \"long\": 0,\n",
        "\n",
        "  # Cluster 1 – Scent\n",
        "  \"smell\": 1,\n",
        "  \"scent\": 1,\n",
        "  \"fragrance\": 1,\n",
        "  \"strong\": 1,\n",
        "  \"packaging\": 1,\n",
        "  \"waste\": 1,\n",
        "  \"money\": 1,\n",
        "  \"heavy\": 1,\n",
        "  \"light\": 1,\n",
        "  \"brand\": 1,\n",
        "  \"complimentary\": 1,\n",
        "\n",
        "  # Cluster 2 – Ineffectiveness\n",
        "  \"didn work\": 2,\n",
        "  \"didn’t help\": 2,\n",
        "  \"tried\": 2,\n",
        "  \"stopped\": 2,\n",
        "\n",
        "  # Cluster 3 – Specific Area or Routine Issues\n",
        "  \"eye\": 3,\n",
        "  \"face\": 3,\n",
        "  \"makeup\": 3,\n",
        "  \"mask\": 3,\n",
        "  \"routine\": 3,\n",
        "  \"spots\": 3,\n",
        "  \"cheeks\": 3,\n",
        "  \"forehead\": 3,\n",
        "  \"breakout\": 3,\n",
        "  \"lips\": 3,\n",
        "\n",
        "  # Cluster 4 – Texture and Hydration Problems\n",
        "  \"dry\": 4,\n",
        "  \"sticky\": 4,\n",
        "  \"greasy\": 4,\n",
        "  \"moisturizer\": 4,\n",
        "  \"texture\": 4,\n",
        "  \"hydrating\": 4,\n",
        "  \"hydration\": 4,\n",
        "  \"oil\": 4,\n",
        "  \"cream\": 4,\n",
        "  \"dried\": 4,\n",
        "  \"balm\": 4,\n",
        "  \"watery\": 4,\n",
        "  \"serum\": 4,\n",
        "\n",
        "  # Cluster 5 – Skin Type Sensitivity\n",
        "  \"acne\": 5,\n",
        "  \"pimples\": 5,\n",
        "  \"bumps\": 5,\n",
        "  \"sensitive skin\": 5,\n",
        "  \"dry skin\": 5,\n",
        "  \"oily skin\": 5,\n",
        "  \"acne prone\": 5\n",
        "}"
      ],
      "metadata": {
        "id": "V0IqWpQeqMsk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stemmer = PorterStemmer()\n",
        "# 建立詞幹對照字典\n",
        "def build_stemmed_keyword_map(keyword_to_tag):\n",
        "  stemmed_map = {}\n",
        "  for phrase, tag in keyword_to_tag.items():\n",
        "    words = phrase.lower().split()\n",
        "    stemmed_phrase = \" \".join(stemmer.stem(word) for word in words)\n",
        "    stemmed_map[stemmed_phrase] = tag\n",
        "  return stemmed_map\n",
        "\n",
        "# 標記函數（詞幹比對版本）\n",
        "def classify_review_tags_with_stemming(text, stemmed_keyword_to_tag):\n",
        "  if not isinstance(text, str) or text.strip() == \"\":\n",
        "    return []\n",
        "  text = text.lower()\n",
        "  words = re.findall(r'\\b\\w+\\b', text)\n",
        "  stemmed_words = [stemmer.stem(w) for w in words]\n",
        "  joined_text = \" \".join(stemmed_words)\n",
        "\n",
        "  tags = set()\n",
        "  for stemmed_phrase, tag in stemmed_keyword_to_tag.items():\n",
        "    if stemmed_phrase in joined_text:\n",
        "      tags.add(tag)\n",
        "  return list(tags)\n",
        "\n",
        "# 合併所有標籤並計算頻率\n",
        "def top_k_labels(tag_lists, k, min_freq):\n",
        "  all_labels = [label for tags in tag_lists for label in tags]\n",
        "  counts = Counter(all_labels)\n",
        "  total = len(all_labels)\n",
        "  freq = {label: count / total for label, count in counts.items()}\n",
        "  selected = [label for label, f in sorted(freq.items(), key=lambda x: x[1], reverse=True) if f >= min_freq][:k]\n",
        "  return selected"
      ],
      "metadata": {
        "id": "-qoVY0Pt5WGx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stemmed_keyword_to_tag_pos = build_stemmed_keyword_map(keyword_to_tag_pos)\n",
        "positive_reviews[\"pos_tags\"] = positive_reviews[\"review_text\"].apply(\n",
        "  lambda text: classify_review_tags_with_stemming(text, stemmed_keyword_to_tag_pos)\n",
        ")\n",
        "positive_reviews['pos_tags'] = positive_reviews['pos_tags'].apply(\n",
        "  lambda x: ast.literal_eval(x) if isinstance(x, str) else x\n",
        ")\n",
        "df_pos_groupby = positive_reviews.groupby('product_id')['pos_tags'].apply(\n",
        "  lambda lists: top_k_labels(lists, k=2, min_freq=0.2)\n",
        ").reset_index()\n",
        "df_pos_groupby = df_pos_groupby[df_pos_groupby['pos_tags'].apply(lambda x: len(x) > 0)].reset_index(drop=True)\n",
        "df_pos = df_pos_groupby.merge(product_info, on='product_id', how='inner')\n",
        "df_pos.to_csv('/content/drive/MyDrive/Colab Notebooks/dataset_final/df_pos.csv', index=False)\n",
        "\n",
        "lengths = df_pos['pos_tags'].apply(len)\n",
        "print('平均主題數:', lengths.mean())\n",
        "print('正面回饋資料數: ', df_pos.shape)\n",
        "df_pos.head()"
      ],
      "metadata": {
        "id": "7IOSczzcCqLc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stemmed_keyword_to_tag_neg = build_stemmed_keyword_map(keyword_to_tag_neg)\n",
        "negative_reviews[\"neg_tags\"] = negative_reviews[\"review_text\"].apply(\n",
        "  lambda text: classify_review_tags_with_stemming(text, stemmed_keyword_to_tag_neg)\n",
        ")\n",
        "negative_reviews['neg_tags'] = negative_reviews['neg_tags'].apply(\n",
        "  lambda x: ast.literal_eval(x) if isinstance(x, str) else x\n",
        ")\n",
        "df_neg_groupby = negative_reviews.groupby('product_id')['neg_tags'].apply(\n",
        "  lambda lists: top_k_labels(lists, k=3, min_freq=0.1)\n",
        ").reset_index()\n",
        "df_neg_groupby = df_neg_groupby[df_neg_groupby['neg_tags'].apply(lambda x: len(x) > 0)].reset_index(drop=True)\n",
        "df_neg = df_neg_groupby.merge(product_info, on='product_id', how='inner')\n",
        "df_neg.to_csv('/content/drive/MyDrive/Colab Notebooks/dataset_final/df_neg.csv', index=False)\n",
        "\n",
        "lengths = df_neg['neg_tags'].apply(len)\n",
        "print('平均主題數:', lengths.mean())\n",
        "print('負面回饋資料數: ', df_neg.shape)\n",
        "df_neg.head()"
      ],
      "metadata": {
        "id": "NPTaC0Kz7uAy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_pos = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/dataset_final/df_pos.csv')\n",
        "df_neg = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/dataset_final/df_neg.csv')"
      ],
      "metadata": {
        "id": "aQ0D_yZmpB-A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -P /content/ 'https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.en.300.bin.gz'\n",
        "!gunzip /content/cc.en.300.bin.gz\n",
        "ft = fasttext.load_model('/content/cc.en.300.bin')"
      ],
      "metadata": {
        "collapsed": true,
        "id": "iUFJDnW5BiAV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Positive dataset"
      ],
      "metadata": {
        "id": "RMFuql65xuIA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "texts = df_pos.apply(\n",
        "  lambda row: ' '.join([\n",
        "    str(row['ingredients']),\n",
        "    str(row['highlights']),\n",
        "    str(row['primary_category']),\n",
        "    str(row['secondary_category']),\n",
        "    str(row['tertiary_category'])\n",
        "  ]), axis=1).values\n",
        "\n",
        "lengths = [len(text.split()) for text in texts]\n",
        "print(f'平均長度: {sum(lengths)/len(lengths):.2f}')\n",
        "print(f'最大長度: {max(lengths)}')\n",
        "print(f'中位數長度: {sorted(lengths)[len(lengths)//2]}')"
      ],
      "metadata": {
        "id": "LHbHt8J-k3Pr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_features = 5000\n",
        "maxlen = 256\n",
        "\n",
        "# 1. 切割訓練集與測試集\n",
        "labels = df_pos['pos_tags'].values\n",
        "texts, labels = shuffle(texts, labels, random_state=100)\n",
        "train_size = 1480\n",
        "test_size = 370\n",
        "texts_train = texts[:train_size]\n",
        "labels_train = labels[:train_size]\n",
        "texts_test = texts[train_size:train_size+test_size]\n",
        "labels_test = labels[train_size:train_size+test_size]\n",
        "\n",
        "# 1.1. 建 tokenizer 並擷取 word_index\n",
        "tokenizer = Tokenizer(num_words=max_features, oov_token=\"<UNK>\")\n",
        "tokenizer.fit_on_texts(texts_train)\n",
        "word_index = tokenizer.word_index\n",
        "\n",
        "# 1.2. Tokenize & pad\n",
        "seq_train = tokenizer.texts_to_sequences(texts_train)\n",
        "x_train = pad_sequences(seq_train, maxlen=maxlen, padding='post', truncating='post')\n",
        "seq_test = tokenizer.texts_to_sequences(texts_test)\n",
        "x_test = pad_sequences(seq_test, maxlen=maxlen, padding='post', truncating='post')\n",
        "\n",
        "# 1.3. 多標籤處理\n",
        "mlb = MultiLabelBinarizer()\n",
        "y_train = mlb.fit_transform(labels_train)\n",
        "y_test = mlb.transform(labels_test)\n",
        "\n",
        "# 2. 讀取 Fasttext 預訓練詞向量\n",
        "# 2.1. 建 embedding matrix\n",
        "embedding_dim = 300\n",
        "embedding_matrix = np.zeros((max_features, embedding_dim))\n",
        "for word, i in word_index.items():\n",
        "  if i < max_features:\n",
        "    embedding_matrix[i] = ft.get_word_vector(word)\n",
        "\n",
        "# 2.2. 做 embedding 層\n",
        "embedding_layer = Embedding(\n",
        "  input_dim=max_features,\n",
        "  output_dim=embedding_dim,\n",
        "  weights=[embedding_matrix],\n",
        "  input_length=maxlen,\n",
        "  trainable=True\n",
        ")"
      ],
      "metadata": {
        "id": "2F7jLGvs0bsY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "covered = np.sum(np.any(embedding_matrix != 0, axis=1))\n",
        "print(f'被 Fasttext 覆蓋的詞數: {covered} / {max_features} = ', covered/max_features)"
      ],
      "metadata": {
        "id": "4Dvb7l7Z-sT9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Negative dataset"
      ],
      "metadata": {
        "id": "X4PrAoZd_rhS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "texts = df_neg.apply(\n",
        "  lambda row: ' '.join([\n",
        "  str(row['ingredients']),\n",
        "  str(row['highlights']),\n",
        "  str(row['primary_category']),\n",
        "  str(row['secondary_category']),\n",
        "  str(row['tertiary_category'])\n",
        "  ]), axis=1).values\n",
        "\n",
        "lengths = [len(text.split()) for text in texts]\n",
        "print(f'平均長度: {sum(lengths)/len(lengths):.2f}')\n",
        "print(f'最大長度: {max(lengths)}')\n",
        "print(f'中位數長度: {sorted(lengths)[len(lengths)//2]}')"
      ],
      "metadata": {
        "id": "nM1iDeQ6kqoT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_features = 5000\n",
        "maxlen = 256\n",
        "\n",
        "# 1. 切割訓練集與測試集\n",
        "labels = df_neg['neg_tags'].values\n",
        "texts, labels = shuffle(texts, labels, random_state=100)\n",
        "train_size = 1400\n",
        "test_size = 350\n",
        "texts_train = texts[:train_size]\n",
        "labels_train = labels[:train_size]\n",
        "texts_test = texts[train_size:train_size+test_size]\n",
        "labels_test = labels[train_size:train_size+test_size]\n",
        "\n",
        "# 1.1. 建 tokenizer 並擷取 word_index\n",
        "tokenizer = Tokenizer(num_words=max_features, oov_token=\"<UNK>\")\n",
        "tokenizer.fit_on_texts(texts_train)\n",
        "word_index = tokenizer.word_index\n",
        "\n",
        "# 1.2. Tokenize & pad\n",
        "seq_train = tokenizer.texts_to_sequences(texts_train)\n",
        "x_train = pad_sequences(seq_train, maxlen=maxlen, padding='post', truncating='post')\n",
        "seq_test = tokenizer.texts_to_sequences(texts_test)\n",
        "x_test = pad_sequences(seq_test, maxlen=maxlen, padding='post', truncating='post')\n",
        "\n",
        "# 1.3. 多標籤處理\n",
        "mlb = MultiLabelBinarizer()\n",
        "y_train = mlb.fit_transform(labels_train)\n",
        "y_test = mlb.transform(labels_test)\n",
        "\n",
        "# 2. 使用 FastText 預訓練詞向量\n",
        "# 2.1. 建 embedding matrix\n",
        "embedding_dim = 300\n",
        "embedding_matrix = np.zeros((max_features, embedding_dim))\n",
        "for word, i in word_index.items():\n",
        "  if i < max_features:\n",
        "    embedding_matrix[i] = ft.get_word_vector(word)\n",
        "\n",
        "# 2.2. 做 embedding 層\n",
        "embedding_layer = Embedding(\n",
        "  input_dim=max_features,\n",
        "  output_dim=embedding_dim,\n",
        "  weights=[embedding_matrix],\n",
        "  input_length=maxlen,\n",
        "  trainable=True\n",
        ")"
      ],
      "metadata": {
        "id": "HZSrfJSmUiVs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "covered = np.sum(np.any(embedding_matrix != 0, axis=1))\n",
        "print(f'被 Fasttext 覆蓋的詞數: {covered} / {max_features} = ', covered/max_features)"
      ],
      "metadata": {
        "id": "TEQO54QIGlBS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model Structure"
      ],
      "metadata": {
        "id": "Fq844Cgo1pOJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_model(cell_type, use_regularization=False, use_dropout=False):\n",
        "  model = Sequential()\n",
        "  model.add(embedding_layer)\n",
        "\n",
        "  if cell_type == 'rnn':\n",
        "    model.add(SimpleRNN(32))\n",
        "  elif cell_type == 'lstm':\n",
        "    model.add(layers.Bidirectional(layers.LSTM(32)))\n",
        "  elif cell_type == 'gru':\n",
        "    model.add(layers.Bidirectional(layers.GRU(32)))\n",
        "\n",
        "  if use_regularization:\n",
        "    model.add(Dense(y_train.shape[1], activation='sigmoid',\n",
        "      kernel_regularizer=regularizers.l1(0.001)))\n",
        "  if use_dropout:\n",
        "    model.add(Dropout(0.5))\n",
        "\n",
        "  model.add(Dense(y_train.shape[1], activation='sigmoid'))\n",
        "  model.compile(optimizer='adam', loss='binary_crossentropy', metrics=[Precision(), Recall(), BinaryAccuracy()])\n",
        "  return model"
      ],
      "metadata": {
        "id": "A54b-I0Bxt6S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model-Positive"
      ],
      "metadata": {
        "id": "0RM1kNfm_06b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A. RNN"
      ],
      "metadata": {
        "id": "v0SJyZO5qROD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = build_model(cell_type='rnn')\n",
        "history_A = model.fit(x_train, y_train, epochs=30, batch_size=32, validation_split=0.1, verbose=0)\n",
        "best_epoch_A = history_A.history['val_loss'].index(min(history_A.history['val_loss']))\n",
        "print('最佳訓練次數：', best_epoch_A+1)"
      ],
      "metadata": {
        "id": "gCeF5-iv_eAN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss = history_A.history['loss']\n",
        "val_loss_values = history_A.history['val_loss']\n",
        "epochs = range(1, len(val_loss_values)+1)\n",
        "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
        "plt.plot(epochs, val_loss_values, 'b', label='Validation Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "JOkyLAWBnwDs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = build_model(cell_type='rnn')\n",
        "model.fit(x_train, y_train, epochs=best_epoch_A+1, batch_size=32, verbose=0)\n",
        "results_A = model.evaluate(x_test, y_test)\n",
        "y_pred_prob = model.predict(x_test)\n",
        "y_pred = (y_pred_prob>0.5).astype(int)\n",
        "micro_A = f1_score(y_test, y_pred, average='micro')\n",
        "macro_A = f1_score(y_test, y_pred, average='macro')\n",
        "print('Micro', f1_score(y_test, y_pred, average='micro'))\n",
        "print('Macro', f1_score(y_test, y_pred, average='macro'))\n",
        "print(classification_report(y_test, y_pred, target_names=[str(x) for x in mlb.classes_]))"
      ],
      "metadata": {
        "id": "MGpYtqnXqW7G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "B. Bidirection LSTM"
      ],
      "metadata": {
        "id": "ztV0BhEwqXTf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = build_model(cell_type='lstm')\n",
        "history_B = model.fit(x_train, y_train, epochs=30, batch_size=32, validation_split=0.1, verbose=0)\n",
        "best_epoch_B = history_B.history['val_loss'].index(min(history_B.history['val_loss']))\n",
        "print('最佳訓練次數：', best_epoch_B+1)"
      ],
      "metadata": {
        "id": "HyfTT63yN7j1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss = history_B.history['loss']\n",
        "val_loss_values = history_B.history['val_loss']\n",
        "epochs = range(1, len(val_loss_values)+1)\n",
        "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
        "plt.plot(epochs, val_loss_values, 'b', label='Validation Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "mWDbvlnLvC__"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = build_model(cell_type='lstm')\n",
        "model.fit(x_train, y_train, epochs=best_epoch_B+1, batch_size=32, verbose=0)\n",
        "results_B = model.evaluate(x_test, y_test)\n",
        "y_pred_prob = model.predict(x_test)\n",
        "y_pred = (y_pred_prob>0.5).astype(int)\n",
        "micro_B = f1_score(y_test, y_pred, average='micro')\n",
        "macro_B = f1_score(y_test, y_pred, average='macro')\n",
        "print('Micro', f1_score(y_test, y_pred, average='micro'))\n",
        "print('Macro', f1_score(y_test, y_pred, average='macro'))\n",
        "print(classification_report(y_test, y_pred, target_names=[str(x) for x in mlb.classes_]))"
      ],
      "metadata": {
        "id": "eAfzvFfSwsDx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "C. Bidirection GRU"
      ],
      "metadata": {
        "id": "g2F1qVeNqa06"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = build_model(cell_type='gru')\n",
        "history_C = model.fit(x_train, y_train, epochs=30, batch_size=32, validation_split=0.1, verbose=0)\n",
        "best_epoch_C = history_C.history['val_loss'].index(min(history_C.history['val_loss']))\n",
        "print('最佳訓練次數：', best_epoch_C+1)"
      ],
      "metadata": {
        "id": "dehTQNAswRc5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss = history_C.history['loss']\n",
        "val_loss_values = history_C.history['val_loss']\n",
        "epochs = range(1, len(val_loss_values)+1)\n",
        "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
        "plt.plot(epochs, val_loss_values, 'b', label='Validation Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "2lfy1Nh2ssLW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = build_model(cell_type='gru')\n",
        "model.fit(x_train, y_train, epochs=best_epoch_C+1, batch_size=32, verbose=0)\n",
        "results_C = model.evaluate(x_test, y_test)\n",
        "y_pred_prob = model.predict(x_test)\n",
        "y_pred = (y_pred_prob>0.5).astype(int)\n",
        "micro_C = f1_score(y_test, y_pred, average='micro')\n",
        "macro_C = f1_score(y_test, y_pred, average='macro')\n",
        "print('Micro', f1_score(y_test, y_pred, average='micro'))\n",
        "print('Macro', f1_score(y_test, y_pred, average='macro'))\n",
        "print(classification_report(y_test, y_pred, target_names=[str(x) for x in mlb.classes_]))"
      ],
      "metadata": {
        "id": "2VtPwbHfqfuC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A, B, C 模型比較"
      ],
      "metadata": {
        "id": "ZYhb9ODm0ANQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_names = ['A', 'B', 'C']\n",
        "testing_loss_values = [results_A[0], results_B[0], results_C[0]]\n",
        "testing_micro = [micro_A, micro_B, micro_C]\n",
        "testing_macro = [macro_A, macro_B, macro_C]\n",
        "\n",
        "df = pd.DataFrame({'Model': model_names,\n",
        "          'Testing Loss': testing_loss_values,\n",
        "          'Testing micro': testing_micro,\n",
        "          'Testing macro': testing_macro})\n",
        "print(df)"
      ],
      "metadata": {
        "id": "CMCSJjmEz-z0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "D. GRU + L1"
      ],
      "metadata": {
        "id": "10wOnUeXQ0ay"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = build_model(cell_type='gru', use_regularization=True)\n",
        "history_D = model.fit(x_train, y_train, epochs=30, batch_size=32, validation_split=0.1, verbose=0)\n",
        "best_epoch_D = history_D.history['val_loss'].index(min(history_D.history['val_loss']))\n",
        "print('最佳訓練次數：', best_epoch_D+1)"
      ],
      "metadata": {
        "id": "gkQ-79s2Qwz8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss = history_D.history['loss']\n",
        "val_loss_values = history_D.history['val_loss']\n",
        "epochs = range(1, len(val_loss_values)+1)\n",
        "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
        "plt.plot(epochs, val_loss_values, 'b', label='Validation Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "25IEU-SQG8Vm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = build_model(cell_type='gru', use_regularization=True)\n",
        "model.fit(x_train, y_train, epochs=best_epoch_D+1, batch_size=32, verbose=0)\n",
        "results_D = model.evaluate(x_test, y_test)\n",
        "y_pred_prob = model.predict(x_test)\n",
        "y_pred = (y_pred_prob>0.5).astype(int)\n",
        "micro_D = f1_score(y_test, y_pred, average='micro')\n",
        "macro_D = f1_score(y_test, y_pred, average='macro')\n",
        "print('Micro', f1_score(y_test, y_pred, average='micro'))\n",
        "print('Macro', f1_score(y_test, y_pred, average='macro'))\n",
        "print(classification_report(y_test, y_pred, target_names=[str(x) for x in mlb.classes_]))"
      ],
      "metadata": {
        "id": "7AtPcHB6Q3lc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "E. GRU + Dropout"
      ],
      "metadata": {
        "id": "9jDFwRpFRBgS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = build_model(cell_type='gru', use_dropout=True)\n",
        "history_E = model.fit(x_train, y_train, epochs=30, batch_size=32, validation_split=0.1, verbose=0)\n",
        "best_epoch_E = history_E.history['val_loss'].index(min(history_E.history['val_loss']))\n",
        "print('最佳訓練次數：', best_epoch_E+1)"
      ],
      "metadata": {
        "id": "3yeee0yKRFc7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss = history_E.history['loss']\n",
        "val_loss_values = history_E.history['val_loss']\n",
        "epochs = range(1, len(val_loss_values)+1)\n",
        "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
        "plt.plot(epochs, val_loss_values, 'b', label='Validation Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "gKZDA1N4HRh6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = build_model(cell_type='gru', use_dropout=True)\n",
        "model.fit(x_train, y_train, epochs=best_epoch_E+1, batch_size=32, verbose=0)\n",
        "results_E = model.evaluate(x_test, y_test)\n",
        "y_pred_prob = model.predict(x_test)\n",
        "y_pred = (y_pred_prob>0.5).astype(int)\n",
        "micro_E = f1_score(y_test, y_pred, average='micro')\n",
        "macro_E = f1_score(y_test, y_pred, average='macro')\n",
        "print('Micro', f1_score(y_test, y_pred, average='micro'))\n",
        "print('Macro', f1_score(y_test, y_pred, average='macro'))\n",
        "print(classification_report(y_test, y_pred, target_names=[str(x) for x in mlb.classes_]))"
      ],
      "metadata": {
        "id": "45QL2KbBRKQj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "C, D, E 模型比較"
      ],
      "metadata": {
        "id": "XuqKmPyXJSd1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_names = ['C', 'D', 'E']\n",
        "testing_loss_values = [results_C[0], results_D[0], results_E[0]]\n",
        "testing_micro = [micro_C, micro_D, micro_E]\n",
        "testing_macro = [macro_C, macro_D, macro_E]\n",
        "\n",
        "df = pd.DataFrame({'Model': model_names,\n",
        "          'Testing Loss': testing_loss_values,\n",
        "          'Testing micro': testing_micro,\n",
        "          'Testing macro': testing_macro})\n",
        "print(df)"
      ],
      "metadata": {
        "id": "4uPEJB32JSG9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model-Negative"
      ],
      "metadata": {
        "id": "VzsPndF8AFMX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A. RNN"
      ],
      "metadata": {
        "id": "ADsaQ3nkASeY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = build_model(cell_type='rnn')\n",
        "history_A = model.fit(x_train, y_train, epochs=30, batch_size=32, validation_split=0.1, verbose=0)\n",
        "best_epoch_A = history_A.history['val_loss'].index(min(history_A.history['val_loss']))\n",
        "print('最佳訓練次數：', best_epoch_A+1)"
      ],
      "metadata": {
        "id": "u-BSOa34AJ6I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss = history_A.history['loss']\n",
        "val_loss_values = history_A.history['val_loss']\n",
        "epochs = range(1, len(val_loss_values)+1)\n",
        "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
        "plt.plot(epochs, val_loss_values, 'b', label='Validation Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "NNX4X8HZAPw3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = build_model(cell_type='rnn')\n",
        "model.fit(x_train, y_train, epochs=best_epoch_A+1, batch_size=32, verbose=0)\n",
        "results_A = model.evaluate(x_test, y_test)\n",
        "y_pred_prob = model.predict(x_test)\n",
        "y_pred = (y_pred_prob>0.5).astype(int)\n",
        "micro_A = f1_score(y_test, y_pred, average='micro')\n",
        "macro_A = f1_score(y_test, y_pred, average='macro')\n",
        "print('Micro', f1_score(y_test, y_pred, average='micro'))\n",
        "print('Macro', f1_score(y_test, y_pred, average='macro'))\n",
        "print(classification_report(y_test, y_pred, target_names=[str(x) for x in mlb.classes_]))"
      ],
      "metadata": {
        "id": "iTTtafD8AQpC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "B. Bidirection LSTM"
      ],
      "metadata": {
        "id": "mXEkFkdYAT6b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = build_model(cell_type='lstm')\n",
        "history_B = model.fit(x_train, y_train, epochs=30, batch_size=32, validation_split=0.1, verbose=0)\n",
        "best_epoch_B = history_B.history['val_loss'].index(min(history_B.history['val_loss']))\n",
        "print('最佳訓練次數：', best_epoch_B+1)"
      ],
      "metadata": {
        "id": "sh6JUYusAXYh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss = history_B.history['loss']\n",
        "val_loss_values = history_B.history['val_loss']\n",
        "epochs = range(1, len(val_loss_values)+1)\n",
        "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
        "plt.plot(epochs, val_loss_values, 'b', label='Validation Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "vqyi-BbmAYFw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = build_model(cell_type='lstm')\n",
        "model.fit(x_train, y_train, epochs=best_epoch_B+1, batch_size=32, verbose=0)\n",
        "results_B = model.evaluate(x_test, y_test)\n",
        "y_pred_prob = model.predict(x_test)\n",
        "y_pred = (y_pred_prob>0.5).astype(int)\n",
        "micro_B = f1_score(y_test, y_pred, average='micro')\n",
        "macro_B = f1_score(y_test, y_pred, average='macro')\n",
        "print('Micro', f1_score(y_test, y_pred, average='micro'))\n",
        "print('Macro', f1_score(y_test, y_pred, average='macro'))\n",
        "print(classification_report(y_test, y_pred, target_names=[str(x) for x in mlb.classes_]))"
      ],
      "metadata": {
        "id": "NrqTYepIHkZb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "C. Bidirection GRU"
      ],
      "metadata": {
        "id": "I10ZYPLlAfeL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = build_model(cell_type='gru')\n",
        "history_C = model.fit(x_train, y_train, epochs=30, batch_size=32, validation_split=0.1, verbose=0)\n",
        "best_epoch_C = history_C.history['val_loss'].index(min(history_C.history['val_loss']))\n",
        "print('最佳訓練次數：', best_epoch_C+1)"
      ],
      "metadata": {
        "id": "nVgdiXzyAi0X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss = history_C.history['loss']\n",
        "val_loss_values = history_C.history['val_loss']\n",
        "epochs = range(1, len(val_loss_values)+1)\n",
        "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
        "plt.plot(epochs, val_loss_values, 'b', label='Validation Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "rl7GZew8Akwr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = build_model(cell_type='gru')\n",
        "model.fit(x_train, y_train, epochs=best_epoch_C+1, batch_size=32, verbose=0)\n",
        "results_C = model.evaluate(x_test, y_test)\n",
        "y_pred_prob = model.predict(x_test)\n",
        "y_pred = (y_pred_prob>0.5).astype(int)\n",
        "micro_C = f1_score(y_test, y_pred, average='micro')\n",
        "macro_C = f1_score(y_test, y_pred, average='macro')\n",
        "print(classification_report(y_test, y_pred, target_names=[str(x) for x in mlb.classes_]))"
      ],
      "metadata": {
        "id": "_2yfLgX3Hl62"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A, B, C 模型比較"
      ],
      "metadata": {
        "id": "FqHs1enxHmns"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_names = ['A', 'B', 'C']\n",
        "testing_loss_values = [results_A[0], results_B[0], results_C[0]]\n",
        "testing_micro = [micro_A, micro_B, micro_C]\n",
        "testing_macro = [macro_A, macro_B, macro_C]\n",
        "\n",
        "df = pd.DataFrame({'Model': model_names,\n",
        "          'Testing Loss': testing_loss_values,\n",
        "          'Testing micro': testing_micro,\n",
        "          'Testing macro': testing_macro})\n",
        "print(df)"
      ],
      "metadata": {
        "id": "8sNggNaNHqlF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "D. RNN + L1"
      ],
      "metadata": {
        "id": "H_9fMAnQHmjw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = build_model(cell_type='rnn', use_regularization=True)\n",
        "history_D = model.fit(x_train, y_train, epochs=30, batch_size=32, validation_split=0.1, verbose=0)\n",
        "best_epoch_D = history_D.history['val_loss'].index(min(history_D.history['val_loss']))\n",
        "print('最佳訓練次數：', best_epoch_D+1)"
      ],
      "metadata": {
        "id": "jboIClQRH9tX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss = history_D.history['loss']\n",
        "val_loss_values = history_D.history['val_loss']\n",
        "epochs = range(1, len(val_loss_values)+1)\n",
        "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
        "plt.plot(epochs, val_loss_values, 'b', label='Validation Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "eCagI-gXH_u5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = build_model(cell_type='rnn', use_regularization=True)\n",
        "model.fit(x_train, y_train, epochs=best_epoch_D+1, batch_size=32, verbose=0)\n",
        "results_D = model.evaluate(x_test, y_test)\n",
        "y_pred_prob = model.predict(x_test)\n",
        "y_pred = (y_pred_prob>0.5).astype(int)\n",
        "micro_D = f1_score(y_test, y_pred, average='micro')\n",
        "macro_D = f1_score(y_test, y_pred, average='macro')\n",
        "print('Micro', f1_score(y_test, y_pred, average='micro'))\n",
        "print('Macro', f1_score(y_test, y_pred, average='macro'))\n",
        "print(classification_report(y_test, y_pred, target_names=[str(x) for x in mlb.classes_]))"
      ],
      "metadata": {
        "id": "DesQaxIDIBji"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "E. RNN + Dropout"
      ],
      "metadata": {
        "id": "O4l9QcTNH5Rd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = build_model(cell_type='rnn', use_dropout=True)\n",
        "history_E = model.fit(x_train, y_train, epochs=30, batch_size=32, validation_split=0.1, verbose=0)\n",
        "best_epoch_E = history_E.history['val_loss'].index(min(history_E.history['val_loss']))\n",
        "print('最佳訓練次數：', best_epoch_E+1)"
      ],
      "metadata": {
        "id": "Ln0vHV93IDNr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss = history_E.history['loss']\n",
        "val_loss_values = history_E.history['val_loss']\n",
        "epochs = range(1, len(val_loss_values)+1)\n",
        "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
        "plt.plot(epochs, val_loss_values, 'b', label='Validation Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "xSdHDPoyIEFn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = build_model(cell_type='rnn', use_dropout=True)\n",
        "model.fit(x_train, y_train, epochs=best_epoch_E+1, batch_size=32, verbose=0)\n",
        "results_E = model.evaluate(x_test, y_test)\n",
        "y_pred_prob = model.predict(x_test)\n",
        "y_pred = (y_pred_prob>0.5).astype(int)\n",
        "micro_E = f1_score(y_test, y_pred, average='micro')\n",
        "macro_E = f1_score(y_test, y_pred, average='macro')\n",
        "print('Micro', f1_score(y_test, y_pred, average='micro'))\n",
        "print('Macro', f1_score(y_test, y_pred, average='macro'))\n",
        "print(classification_report(y_test, y_pred, target_names=[str(x) for x in mlb.classes_]))"
      ],
      "metadata": {
        "id": "6tMlFpj_IFKB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A, D, E 模型比較"
      ],
      "metadata": {
        "id": "fLw3ex-TJbkt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_names = ['A', 'D', 'E']\n",
        "testing_loss_values = [results_A[0], results_D[0], results_E[0]]\n",
        "testing_micro = [micro_A, micro_D, micro_E]\n",
        "testing_macro = [macro_A, macro_D, macro_E]\n",
        "\n",
        "df = pd.DataFrame({'Model': model_names,\n",
        "          'Testing Loss': testing_loss_values,\n",
        "          'Testing micro': testing_micro,\n",
        "          'Testing macro': testing_macro})\n",
        "print(df)"
      ],
      "metadata": {
        "id": "COmm6aBAJbLd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}